{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "zip = ZipFile('Resources/archive.zip')\n",
    "zip.extractall('Resources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display all the columns (to see which to drop)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Mutual Fund informaiton csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CSV data \n",
    "#MutualFunds\n",
    "mutualFunds= pd.read_csv(\n",
    "    Path(\"Resources/MutualFunds.csv\")\n",
    ",index_col=\"fund_symbol\")\n",
    "mutualFunds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with only 1 value to drop\n",
    "mutualFunds.loc[: , mutualFunds.dtypes== \"object\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dates to date\n",
    "mutualFunds[\"inception_date\"] = pd.to_datetime(mutualFunds[\"inception_date\"])\n",
    "mutualFunds[\"management_start_date\"] = pd.to_datetime(mutualFunds[\"management_start_date\"])\n",
    "mutualFunds[\"returns_as_of_date\"] = pd.to_datetime(mutualFunds[\"returns_as_of_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds_counts= mutualFunds.loc[: ,mutualFunds.dtypes==\"object\"].nunique()\n",
    "mutualFunds_counts_one= mutualFunds_counts[mutualFunds_counts == 1].index.to_list()\n",
    "print(mutualFunds_counts_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns with only 1 unique value\n",
    "mutualFunds.drop(columns=mutualFunds_counts_one,inplace=True)\n",
    "mutualFunds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We noticed that there are many `NaN` values throughout the data set. We will now explore which columns/rows have NaN and will remove an appropriate amount of columns/rows._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we drop rows that have an `NaN`, then all data is removed. Instead, let's look at the columns that have missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at each column and the count of NaN in each column\n",
    "list = []\n",
    "for index, row in pd.DataFrame(mutualFunds.isna().sum()).iterrows():\n",
    "    list.append((index,row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Missing Values\n",
    "# Fill missing values in numerical columns with mean or median\n",
    "numerical_cols = mutualFunds.select_dtypes(include=['number']).columns\n",
    "mutualFunds[numerical_cols] = mutualFunds[numerical_cols].fillna(mutualFunds[numerical_cols].mean())\n",
    "\n",
    "# Fill missing values in categorical columns with mode\n",
    "categorical_cols = mutualFunds.select_dtypes(include=['object']).columns\n",
    "mutualFunds[categorical_cols] = mutualFunds[categorical_cols].fillna(mutualFunds[categorical_cols].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Categorical Variables\n",
    "# One-hot encode categorical variables\n",
    "mutualFunds_encoded = pd.get_dummies(mutualFunds, columns=categorical_cols)\n",
    "\n",
    "# Scaling Numerical Features\n",
    "# Separate numerical columns for scaling\n",
    "numerical_cols = mutualFunds_encoded.select_dtypes(include=['number']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan to find the number of columns that have above a certain percentage of `NaN` and then remove those columns. We have chosen 60% so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6*len(mutualFunds)\n",
    "drop_columns = []\n",
    "for i in range(len(list)):\n",
    "    if list[i][1][0] >= threshold:\n",
    "        drop_columns.append(list[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(drop_columns))\n",
    "drop_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds.drop(columns=drop_columns, inplace = True)\n",
    "mutualFunds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with only 1 value to drop\n",
    "mutualFunds.loc[: , mutualFunds.dtypes== \"object\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary columns\n",
    "mutualFunds.drop(columns=[\"fund_short_name\", \"fund_long_name\",\"management_name\", \"management_bio\", \"investment_strategy\"],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove quarterly data\n",
    "mutualFunds.drop(columns=[\"fund_return_2021_q1\",\n",
    "\"fund_return_2020_q4\",\"fund_return_2020_q3\",\"fund_return_2020_q2\",\"fund_return_2020_q1\",\n",
    "\"fund_return_2019_q4\",\"fund_return_2019_q3\",\"fund_return_2019_q2\",\"fund_return_2019_q1\",\n",
    "\"fund_return_2018_q4\",\"fund_return_2018_q3\",\"fund_return_2018_q2\",\"fund_return_2018_q1\",\n",
    "\"fund_return_2017_q4\",\"fund_return_2017_q3\",\"fund_return_2017_q2\",\"fund_return_2017_q1\",\n",
    "\"fund_return_2016_q4\",\"fund_return_2016_q3\",\"fund_return_2016_q2\",\"fund_return_2016_q1\",\n",
    "\"fund_return_2015_q4\",\"fund_return_2015_q3\",\"fund_return_2015_q2\",\"fund_return_2015_q1\",\n",
    "\"fund_return_2014_q4\",\"fund_return_2014_q3\",\"fund_return_2014_q2\",\"fund_return_2014_q1\",\n",
    "\"fund_return_2013_q4\",\"fund_return_2013_q3\",\"fund_return_2013_q2\",\"fund_return_2013_q1\",\n",
    "\"fund_return_2012_q4\",\"fund_return_2012_q3\",\"fund_return_2012_q2\",\"fund_return_2012_q1\",\n",
    "\"fund_return_2011_q4\",\"fund_return_2011_q3\",\"fund_return_2011_q2\",\"fund_return_2011_q1\",\n",
    "\"fund_return_2010_q4\",\"fund_return_2010_q3\",\"fund_return_2010_q2\",\"fund_return_2010_q1\",\n",
    "\"fund_return_2009_q4\",\"fund_return_2009_q3\",\"fund_return_2009_q2\",\"fund_return_2009_q1\",\n",
    "\"fund_return_2008_q3\",\"fund_return_2008_q2\",\"fund_return_2008_q1\",\n",
    "\"fund_alpha_3years\",\"fund_beta_3years\",\"fund_mean_annual_return_3years\",\"fund_r_squared_3years\",\"fund_stdev_3years\",\"fund_sharpe_ratio_3years\",\"fund_treynor_ratio_3years\",\n",
    "\"fund_alpha_5years\",\"fund_beta_5years\",\"fund_mean_annual_return_5years\",\"fund_r_squared_5years\",\"fund_stdev_5years\",\"fund_sharpe_ratio_5years\",\"fund_treynor_ratio_5years\",\n",
    "\"fund_alpha_10years\",\"fund_beta_10years\",\"fund_mean_annual_return_10years\",\"fund_r_squared_10years\",\"fund_stdev_10years\",\"fund_sharpe_ratio_10years\",\"fund_treynor_ratio_10years\",\n",
    "\"fund_return_category_rank_ytd\",\"fund_return_category_rank_1month\",\"fund_return_category_rank_3months\",\"fund_return_category_rank_1year\",\"fund_return_category_rank_3years\",\n",
    "\"fund_return_category_rank_5years\",\"load_adj_return_1year\",\"load_adj_return_3years\",\"load_adj_return_5years\",\"load_adj_return_10years\",\n",
    "\"top10_holdings\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove metrics that we can caluclate ourselves\n",
    "#keep: \"esg_score\",\"environment_score\", \"sustainability_score\", \"sustainability_rank\",   \"social_score\",  \"governance_score\", \n",
    "mutualFunds.drop(columns=[\"esg_peer_count\",\"peer_esg_min\", \"peer_esg_avg\", \"peer_esg_max\",\n",
    "\"peer_environment_min\", \"peer_environment_avg\", \"peer_environment_max\", \n",
    "\"peer_social_min\", \"peer_social_avg\", \"peer_social_max\",\n",
    "\"peer_governance_min\", \"peer_governance_avg\", \"peer_governance_max\"],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds.loc[: , mutualFunds.dtypes== \"object\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done with Data cleanup and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average for each column\n",
    "column_means = mutualFunds.mean(skipna=True, numeric_only=True)\n",
    "# Replace NaN values in each column with the respective column average\n",
    "mutualFunds.fillna(column_means, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds.dropna(inplace= True)\n",
    "mutualFunds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds.loc[:,mutualFunds.dtypes == \"object\"].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning `fund_category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_category_type_count = mutualFunds[\"fund_category\"].value_counts()[mutualFunds[\"fund_category\"].value_counts() > 150]\n",
    "print(100*fund_category_type_count.sum()/len(mutualFunds))\n",
    "print(fund_category_type_count.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in mutualFunds[\"fund_category\"]:\n",
    "    if cat in fund_category_type_count.index.to_list():\n",
    "        next\n",
    "    else:\n",
    "        mutualFunds[\"fund_category\"] = mutualFunds[\"fund_category\"].replace(cat, \"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds[\"fund_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning `fund_family_type_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_family_type_count = mutualFunds[\"fund_family\"].value_counts()[mutualFunds[\"fund_family\"].value_counts() > 150]\n",
    "print(100*fund_family_type_count.sum()/len(mutualFunds))\n",
    "print(fund_family_type_count.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in mutualFunds[\"fund_family\"]:\n",
    "    if cat in fund_family_type_count.index.to_list():\n",
    "        next\n",
    "    else:\n",
    "        mutualFunds[\"fund_family\"] = mutualFunds[\"fund_family\"].replace(cat, \"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds[\"fund_family\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning `esg_peer_group`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding threshhold to bin esg_peer_group\n",
    "esg_peer_type_count = mutualFunds[\"esg_peer_group\"].value_counts()[mutualFunds[\"esg_peer_group\"].value_counts() > 100]\n",
    "print(100* esg_peer_type_count.sum()/len(mutualFunds))\n",
    "print(esg_peer_type_count.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the bins of esg_peer_group\n",
    "esg_peer_type_count.index.to_list()\n",
    "#replace bins in dataframe for esg_peer_group\n",
    "for esg in mutualFunds[\"esg_peer_group\"]:\n",
    "    if esg in esg_peer_type_count.index.to_list():\n",
    "        next\n",
    "    else:\n",
    "        mutualFunds[\"esg_peer_group\"] = mutualFunds[\"esg_peer_group\"].replace(esg, \"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds[\"esg_peer_group\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move the cleaned and binned mutualFunds CSV to a new CSV\n",
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "output_directory = \"Resources/Cleaned\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Now you can save the CSV file\n",
    "binned_mutual_funds = mutualFunds.copy()\n",
    "output_file_path = os.path.join(output_directory, \"binned_mutual_funds.csv\")\n",
    "binned_mutual_funds.to_csv(output_file_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read In CSV data for Mutual Fund prices A-Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CSV data \n",
    "#MutualFund prices A-E\n",
    "df_AE= pd.read_csv(\n",
    "    Path(\"Resources/MutualFund Prices - A-E.csv\")\n",
    ")\n",
    "df_AE.head()\n",
    "\n",
    "#set index\n",
    "df_AE.set_index(\"fund_symbol\", inplace=True)\n",
    "df_AE[\"price_date\"]=pd.to_datetime(df_AE['price_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CSV data \n",
    "#MutualFund prices F-K\n",
    "df_FK= pd.read_csv(\n",
    "    Path(\"Resources/MutualFund Prices - F-K.csv\")\n",
    ")\n",
    "df_FK.head()\n",
    "\n",
    "#set index\n",
    "df_FK.set_index(\"fund_symbol\", inplace=True)\n",
    "df_FK[\"price_date\"]=pd.to_datetime(df_FK['price_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CSV data \n",
    "#MutualFund prices L-P\n",
    "df_LP= pd.read_csv(\n",
    "    Path(\"Resources/MutualFund Prices - L-P.csv\")\n",
    ")\n",
    "df_LP.head()\n",
    "\n",
    "#set index\n",
    "df_LP.set_index(\"fund_symbol\", inplace=True)\n",
    "df_LP[\"price_date\"]=pd.to_datetime(df_LP['price_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CSV data \n",
    "#MutualFund prices Q-Z\n",
    "df_QZ= pd.read_csv(\n",
    "    Path(\"Resources/MutualFund Prices - Q-Z.csv\")\n",
    ")\n",
    "df_QZ.head()\n",
    "\n",
    "#set index\n",
    "df_QZ.set_index(\"fund_symbol\", inplace=True)\n",
    "df_QZ[\"price_date\"]=pd.to_datetime(df_QZ['price_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Concatonate dataframes\n",
    "mutual_fund_df= pd.concat([df_AE,df_FK,df_LP,df_QZ])\n",
    "mutual_fund_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if price_date datatype has successfully changed\n",
    "mutual_fund_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the cleaned and concatonated mutual_fund_df DataFrame\n",
    "az_mutual_funds = mutual_fund_df.copy()\n",
    "\n",
    "# Define the new file name and path where you want to save the CSV file\n",
    "output_file_path_2 = \"Resources/Cleaned/az_mutual_funds.csv\"\n",
    "\n",
    "# Write the DataFrame to CSV\n",
    "az_mutual_funds.to_csv(output_file_path_2, header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge mutualFunds and mutual_fund_df on index= \"fund_symbol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType,StructField,StringType, DateType,IntegerType\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths for the cleaned CSV files\n",
    "az_mutual_funds_file_path = \"Resources/Cleaned/az_mutual_funds.csv\"\n",
    "binned_mutual_funds_file_path = \"Resources/Cleaned/binned_mutual_funds.csv\"\n",
    "\n",
    "# Read the CSV files into Spark DataFrames\n",
    "az_mutual_funds_df = spark.read.csv(az_mutual_funds_file_path, header=True, inferSchema=True)\n",
    "binned_mutual_funds_df = spark.read.csv(binned_mutual_funds_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrames\n",
    "az_mutual_funds_df.show()\n",
    "binned_mutual_funds_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame in Parquet format with compression and overwrite existing files\n",
    "az_mutual_funds_df.write.parquet(\"path/to/az_mutual_funds_parquet\", mode=\"overwrite\")\n",
    "binned_mutual_funds_df.write.parquet(\"path/to/binned_mutual_funds_parquet\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet files into Spark DataFrame\n",
    "az_mutual_funds_df = spark.read.parquet(\"path/to/az_mutual_funds_parquet\")\n",
    "binned_mutual_funds_df = spark.read.parquet(\"path/to/binned_mutual_funds_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrames as temporary views\n",
    "az_mutual_funds_df.createOrReplaceTempView(\"az_mutual_funds\")\n",
    "binned_mutual_funds_df.createOrReplaceTempView(\"binned_mutual_funds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SQL merge \n",
    "merged_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM az_mutual_funds\n",
    "    INNER JOIN binned_mutual_funds\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post merging and binning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to list of lists\n",
    "data = mutualFunds.values.tolist()\n",
    "\n",
    "# Get the column names from the Pandas DataFrame\n",
    "columns = mutualFunds.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the Spark DataFrame\n",
    "schema = StructType([\n",
    "    StructField(name, StringType(), nullable=True)  # Adjust StringType() as needed\n",
    "    for name in columns\n",
    "])\n",
    "\n",
    "# Create the Spark DataFrame\n",
    "mutualFunds_spark = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the target column 'year_to_date_return'\n",
    "y = mutualFunds[\"year_to_date_return\"]\n",
    "\n",
    "# Select all columns except 'year_to_date_return' as features\n",
    "X = mutualFunds.drop(columns=[\"year_to_date_return\"])\n",
    "\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify numeric, categorical, and datetime columns\n",
    "numeric_cols = [col for col, dtype in X_train.dtypes.items() if dtype in ['int', 'float']]\n",
    "categorical_cols = [col for col, dtype in X_train.dtypes.items() if dtype == 'object']\n",
    "# Assuming datetime columns are identified by a specific string pattern in the column name\n",
    "datetime_cols = [col for col in X_train.columns if 'date' in col.lower()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Identify numeric, categorical, and datetime columns\n",
    "numeric_cols = X_train.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "datetime_cols = X_train.select_dtypes(include=['datetime']).columns\n",
    "\n",
    "# Create transformers for numeric and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocess numeric and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Define a function to convert datetime columns to timestamps\n",
    "def datetime_to_timestamp(X):\n",
    "    for col in datetime_cols:\n",
    "        X[col] = X[col].apply(lambda x: x.timestamp())\n",
    "    return X\n",
    "\n",
    "# Create a transformer to apply the conversion\n",
    "datetime_transformer = FunctionTransformer(datetime_to_timestamp)\n",
    "\n",
    "# Modify the preprocessor to include the datetime transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        ('datetime', datetime_transformer, datetime_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the preprocessor on the training data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the fitted preprocessor\n",
    "X_test_preprocessed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "number_input_features = X_train_preprocessed.shape[1]\n",
    "hidden_nodes_layer1 = 64\n",
    "hidden_nodes_layer2 = 32\n",
    "\n",
    "nn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),  # Add batch normalization\n",
    "    tf.keras.layers.Dropout(0.2),  # Add dropout layer for regularization\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile, Train, Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print model summary\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = nn.fit(X_train_preprocessed, y_train, epochs=50, batch_size=32, validation_data=(X_test_preprocessed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_preprocessed, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
