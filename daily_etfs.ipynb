{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "zip = ZipFile('Resources/archive.zip')\n",
    "zip.extractall('Resources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display all the columns (to see which to drop)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Mutual Fund informaiton csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CSV data \n",
    "#MutualFunds\n",
    "mutualFunds= pd.read_csv(\n",
    "    Path(\"Resources/MutualFunds.csv\")\n",
    ",index_col=\"fund_symbol\")\n",
    "mutualFunds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with only 1 value to drop\n",
    "mutualFunds.loc[: , mutualFunds.dtypes== \"object\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dates to date\n",
    "mutualFunds[\"inception_date\"] = pd.to_datetime(mutualFunds[\"inception_date\"])\n",
    "mutualFunds[\"management_start_date\"] = pd.to_datetime(mutualFunds[\"management_start_date\"])\n",
    "mutualFunds[\"returns_as_of_date\"] = pd.to_datetime(mutualFunds[\"returns_as_of_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds_counts= mutualFunds.loc[: ,mutualFunds.dtypes==\"object\"].nunique()\n",
    "mutualFunds_counts_one= mutualFunds_counts[mutualFunds_counts == 1].index.to_list()\n",
    "print(mutualFunds_counts_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns with only 1 unique value\n",
    "mutualFunds.drop(columns=mutualFunds_counts_one,inplace=True)\n",
    "mutualFunds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We noticed that there are many `NaN` values throughout the data set. We will now explore which columns/rows have NaN and will remove an appropriate amount of columns/rows._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we drop rows that have an `NaN`, then all data is removed. Instead, let's look at the columns that have missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at each column and the count of NaN in each column\n",
    "list = []\n",
    "for index, row in pd.DataFrame(mutualFunds.isna().sum()).iterrows():\n",
    "    list.append((index,row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan to find the number of columns that have above a certain percentage of `NaN` and then remove those columns. We have chosen 60% so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6*len(mutualFunds)\n",
    "drop_columns = []\n",
    "for i in range(len(list)):\n",
    "    if list[i][1][0] >= threshold:\n",
    "        drop_columns.append(list[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(drop_columns))\n",
    "drop_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds.drop(columns=drop_columns, inplace = True)\n",
    "mutualFunds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with only 1 value to drop\n",
    "mutualFunds.loc[: , mutualFunds.dtypes== \"object\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary columns\n",
    "mutualFunds.drop(columns=[\"fund_short_name\", \"fund_long_name\",\"management_name\", \"management_bio\", \"investment_strategy\"],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove quarterly data\n",
    "mutualFunds.drop(columns=[\"fund_return_2021_q1\",\n",
    "\"fund_return_2020_q4\",\"fund_return_2020_q3\",\"fund_return_2020_q2\",\"fund_return_2020_q1\",\n",
    "\"fund_return_2019_q4\",\"fund_return_2019_q3\",\"fund_return_2019_q2\",\"fund_return_2019_q1\",\n",
    "\"fund_return_2018_q4\",\"fund_return_2018_q3\",\"fund_return_2018_q2\",\"fund_return_2018_q1\",\n",
    "\"fund_return_2017_q4\",\"fund_return_2017_q3\",\"fund_return_2017_q2\",\"fund_return_2017_q1\",\n",
    "\"fund_return_2016_q4\",\"fund_return_2016_q3\",\"fund_return_2016_q2\",\"fund_return_2016_q1\",\n",
    "\"fund_return_2015_q4\",\"fund_return_2015_q3\",\"fund_return_2015_q2\",\"fund_return_2015_q1\",\n",
    "\"fund_return_2014_q4\",\"fund_return_2014_q3\",\"fund_return_2014_q2\",\"fund_return_2014_q1\",\n",
    "\"fund_return_2013_q4\",\"fund_return_2013_q3\",\"fund_return_2013_q2\",\"fund_return_2013_q1\",\n",
    "\"fund_return_2012_q4\",\"fund_return_2012_q3\",\"fund_return_2012_q2\",\"fund_return_2012_q1\",\n",
    "\"fund_return_2011_q4\",\"fund_return_2011_q3\",\"fund_return_2011_q2\",\"fund_return_2011_q1\",\n",
    "\"fund_return_2010_q4\",\"fund_return_2010_q3\",\"fund_return_2010_q2\",\"fund_return_2010_q1\",\n",
    "\"fund_return_2009_q4\",\"fund_return_2009_q3\",\"fund_return_2009_q2\",\"fund_return_2009_q1\",\n",
    "\"fund_return_2008_q3\",\"fund_return_2008_q2\",\"fund_return_2008_q1\",\n",
    "\"fund_alpha_3years\",\"fund_beta_3years\",\"fund_mean_annual_return_3years\",\"fund_r_squared_3years\",\"fund_stdev_3years\",\"fund_sharpe_ratio_3years\",\"fund_treynor_ratio_3years\",\n",
    "\"fund_alpha_5years\",\"fund_beta_5years\",\"fund_mean_annual_return_5years\",\"fund_r_squared_5years\",\"fund_stdev_5years\",\"fund_sharpe_ratio_5years\",\"fund_treynor_ratio_5years\",\n",
    "\"fund_alpha_10years\",\"fund_beta_10years\",\"fund_mean_annual_return_10years\",\"fund_r_squared_10years\",\"fund_stdev_10years\",\"fund_sharpe_ratio_10years\",\"fund_treynor_ratio_10years\",\n",
    "\"fund_return_category_rank_ytd\",\"fund_return_category_rank_1month\",\"fund_return_category_rank_3months\",\"fund_return_category_rank_1year\",\"fund_return_category_rank_3years\",\n",
    "\"fund_return_category_rank_5years\",\"load_adj_return_1year\",\"load_adj_return_3years\",\"load_adj_return_5years\",\"load_adj_return_10years\",\n",
    "\"top10_holdings\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove metrics that we can caluclate ourselves\n",
    "#keep: \"esg_score\",\"environment_score\", \"sustainability_score\", \"sustainability_rank\",   \"social_score\",  \"governance_score\", \n",
    "mutualFunds.drop(columns=[\"esg_peer_count\",\"peer_esg_min\", \"peer_esg_avg\", \"peer_esg_max\",\n",
    "\"peer_environment_min\", \"peer_environment_avg\", \"peer_environment_max\", \n",
    "\"peer_social_min\", \"peer_social_avg\", \"peer_social_max\",\n",
    "\"peer_governance_min\", \"peer_governance_avg\", \"peer_governance_max\"],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds.loc[: , mutualFunds.dtypes== \"object\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done with Data cleanup and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average for each column\n",
    "column_means = mutualFunds.mean(skipna=True, numeric_only=True)\n",
    "# Replace NaN values in each column with the respective column average\n",
    "mutualFunds.fillna(column_means, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds.dropna(inplace= True)\n",
    "mutualFunds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds.loc[:,mutualFunds.dtypes == \"object\"].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning `fund_category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_category_type_count = mutualFunds[\"fund_category\"].value_counts()[mutualFunds[\"fund_category\"].value_counts() > 150]\n",
    "print(100*fund_category_type_count.sum()/len(mutualFunds))\n",
    "print(fund_category_type_count.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in mutualFunds[\"fund_category\"]:\n",
    "    if cat in fund_category_type_count.index.to_list():\n",
    "        next\n",
    "    else:\n",
    "        mutualFunds[\"fund_category\"] = mutualFunds[\"fund_category\"].replace(cat, \"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds[\"fund_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning `fund_family_type_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_family_type_count = mutualFunds[\"fund_family\"].value_counts()[mutualFunds[\"fund_family\"].value_counts() > 150]\n",
    "print(100*fund_family_type_count.sum()/len(mutualFunds))\n",
    "print(fund_family_type_count.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in mutualFunds[\"fund_family\"]:\n",
    "    if cat in fund_family_type_count.index.to_list():\n",
    "        next\n",
    "    else:\n",
    "        mutualFunds[\"fund_family\"] = mutualFunds[\"fund_family\"].replace(cat, \"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds[\"fund_family\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning `esg_peer_group`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding threshhold to bin esg_peer_group\n",
    "esg_peer_type_count = mutualFunds[\"esg_peer_group\"].value_counts()[mutualFunds[\"esg_peer_group\"].value_counts() > 100]\n",
    "print(100* esg_peer_type_count.sum()/len(mutualFunds))\n",
    "print(esg_peer_type_count.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the bins of esg_peer_group\n",
    "esg_peer_type_count.index.to_list()\n",
    "#replace bins in dataframe for esg_peer_group\n",
    "for esg in mutualFunds[\"esg_peer_group\"]:\n",
    "    if esg in esg_peer_type_count.index.to_list():\n",
    "        next\n",
    "    else:\n",
    "        mutualFunds[\"esg_peer_group\"] = mutualFunds[\"esg_peer_group\"].replace(esg, \"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds[\"esg_peer_group\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move the cleaned and binned mutualFunds CSV to a new CSV\n",
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "output_directory = \"Resources/Cleaned\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Now you can save the CSV file\n",
    "binned_mutual_funds = mutualFunds.copy()\n",
    "output_file_path = os.path.join(output_directory, \"binned_mutual_funds.csv\")\n",
    "binned_mutual_funds.to_csv(output_file_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read In CSV data for Mutual Fund prices A-Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CSV data \n",
    "#MutualFund prices A-E\n",
    "df_AE= pd.read_csv(\n",
    "    Path(\"Resources/MutualFund Prices - A-E.csv\")\n",
    ")\n",
    "df_AE.head()\n",
    "\n",
    "#set index\n",
    "df_AE.set_index(\"fund_symbol\", inplace=True)\n",
    "df_AE[\"price_date\"]=pd.to_datetime(df_AE['price_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CSV data \n",
    "#MutualFund prices F-K\n",
    "df_FK= pd.read_csv(\n",
    "    Path(\"Resources/MutualFund Prices - F-K.csv\")\n",
    ")\n",
    "df_FK.head()\n",
    "\n",
    "#set index\n",
    "df_FK.set_index(\"fund_symbol\", inplace=True)\n",
    "df_FK[\"price_date\"]=pd.to_datetime(df_FK['price_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CSV data \n",
    "#MutualFund prices L-P\n",
    "df_LP= pd.read_csv(\n",
    "    Path(\"Resources/MutualFund Prices - L-P.csv\")\n",
    ")\n",
    "df_LP.head()\n",
    "\n",
    "#set index\n",
    "df_LP.set_index(\"fund_symbol\", inplace=True)\n",
    "df_LP[\"price_date\"]=pd.to_datetime(df_LP['price_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CSV data \n",
    "#MutualFund prices Q-Z\n",
    "df_QZ= pd.read_csv(\n",
    "    Path(\"Resources/MutualFund Prices - Q-Z.csv\")\n",
    ")\n",
    "df_QZ.head()\n",
    "\n",
    "#set index\n",
    "df_QZ.set_index(\"fund_symbol\", inplace=True)\n",
    "df_QZ[\"price_date\"]=pd.to_datetime(df_QZ['price_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Concatonate dataframes\n",
    "mutual_fund_df= pd.concat([df_AE,df_FK,df_LP,df_QZ])\n",
    "mutual_fund_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if price_date datatype has successfully changed\n",
    "mutual_fund_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the cleaned and concatonated mutual_fund_df DataFrame\n",
    "az_mutual_funds = mutual_fund_df.copy()\n",
    "\n",
    "# Define the new file name and path where you want to save the CSV file\n",
    "output_file_path_2 = \"Resources/Cleaned/az_mutual_funds.csv\"\n",
    "\n",
    "# Write the DataFrame to CSV\n",
    "az_mutual_funds.to_csv(output_file_path_2, header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge mutualFunds and mutual_fund_df on index= \"fund_symbol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType,StructField,StringType, DateType,IntegerType\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths for the cleaned CSV files\n",
    "az_mutual_funds_file_path = \"Resources/Cleaned/az_mutual_funds.csv\"\n",
    "binned_mutual_funds_file_path = \"Resources/Cleaned/binned_mutual_funds.csv\"\n",
    "\n",
    "# Read the CSV files into Spark DataFrames\n",
    "az_mutual_funds_df = spark.read.csv(az_mutual_funds_file_path, header=True, inferSchema=True)\n",
    "binned_mutual_funds_df = spark.read.csv(binned_mutual_funds_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrames\n",
    "az_mutual_funds_df.show()\n",
    "binned_mutual_funds_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame in Parquet format with compression and overwrite existing files\n",
    "az_mutual_funds_df.write.parquet(\"path/to/az_mutual_funds_parquet\", mode=\"overwrite\")\n",
    "binned_mutual_funds_df.write.parquet(\"path/to/binned_mutual_funds_parquet\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read compressed Parquet files into Spark DataFrame\n",
    "az_mutual_funds_df = spark.read.parquet(\"path/to/az_mutual_funds_parquet\")\n",
    "binned_mutual_funds_df = spark.read.parquet(\"path/to/binned_mutual_funds_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrames as temporary views\n",
    "az_mutual_funds_df.createOrReplaceTempView(\"az_mutual_funds\")\n",
    "binned_mutual_funds_df.createOrReplaceTempView(\"binned_mutual_funds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SQL merge without decompressing\n",
    "merged_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM az_mutual_funds\n",
    "    INNER JOIN binned_mutual_funds\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post merging and binning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualFunds_spark = spark.createDataFrame(mutualFunds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target array\n",
    "y = mutualFunds.select(\"year_to_date_return\")\n",
    "X = mutualFunds.drop(\"year_to_date_return\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = y.toPandas()\n",
    "X = X.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the preprocessed data into training and testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating StandardScaler instance\n",
    "scaler= StandardScaler()\n",
    "# Fitting Standard Scaler\n",
    "x_scaler= scaler.fit(X_train)\n",
    "# Scaling Data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 80\n",
    "hidden_nodes_layer2 = 30\n",
    "\n",
    "nn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile, Train, Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
